{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocess\n",
    "import vectorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam = pd.read_csv(\"steam_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funny</th>\n",
       "      <th>helpful</th>\n",
       "      <th>hour_played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.348910e+05</td>\n",
       "      <td>434891.000000</td>\n",
       "      <td>434891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.333024e+05</td>\n",
       "      <td>1.004114</td>\n",
       "      <td>364.130773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.785640e+07</td>\n",
       "      <td>59.462935</td>\n",
       "      <td>545.961198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.294967e+09</td>\n",
       "      <td>28171.000000</td>\n",
       "      <td>31962.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              funny        helpful    hour_played\n",
       "count  4.348910e+05  434891.000000  434891.000000\n",
       "mean   5.333024e+05       1.004114     364.130773\n",
       "std    4.785640e+07      59.462935     545.961198\n",
       "min    0.000000e+00       0.000000       0.000000\n",
       "25%    0.000000e+00       0.000000      62.000000\n",
       "50%    0.000000e+00       0.000000     190.000000\n",
       "75%    0.000000e+00       0.000000     450.000000\n",
       "max    4.294967e+09   28171.000000   31962.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam[\"helpful_binary\"] = steam[\"helpful\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [&gt, Plai, German, Reich&gt, Declar, war, Bel...\n",
      "1                                                   [ye, .]\n",
      "2         [Veri, good, game, although, bit, overpr, opin...\n",
      "3         [Out, review, I, wrote, Thi, on, probabl, most...\n",
      "4         [Disclaim, I, survivor, main, ., I, plai, game...\n",
      "                                ...                        \n",
      "434886    [YOUR, FLESH, WILL, ROT, AND, DECAY, ., STEEL,...\n",
      "434887    [Domini, Domina, I, believ, we, deal, jewel, ....\n",
      "434888    [First, off, if, like, X, Com, style, game, mo...\n",
      "434889    [As, disclaim, I'm, AdMech, player, tabletop, ...\n",
      "434890    [Don't, listen, peopl, claim, it', game, onli,...\n",
      "Length: 434891, dtype: object\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "for line in steam[\"review\"]:\n",
    "    line = str(line)\n",
    "    text = preprocess.removeSGML(line)\n",
    "    text = preprocess.tokenizeText(text)\n",
    "    text = preprocess.removeStopwords(text)\n",
    "    text = preprocess.stemWords(text)\n",
    "    arr.append(text)\n",
    "\n",
    "preprocessed = pd.Series(arr)\n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam[\"reviews_processed\"] = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/theohaddad/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/theohaddad/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/theohaddad/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/theohaddad/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifer():\n",
    "    def __init__(self, k=1, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        y_predict = []\n",
    "\n",
    "        for i in range(len(x_test)):\n",
    "            max_sim = 0\n",
    "            max_index = 0\n",
    "            for j in range(len(self.x_train)):\n",
    "                temp = self.similarity(x_test[i], self.x_train[j])\n",
    "                if temp > max_sim:\n",
    "                    max_sim = temp\n",
    "                    max_index = j\n",
    "            y_predict.append(self.y_train[max_index])\n",
    "        return y_predict\n",
    "\n",
    "    def similarity(self,doc1, doc2):\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "        \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(steam[\"reviews_processed\"][:200000])\n",
    "Y_train = list(steam[\"recommendation\"][:200000])\n",
    "\n",
    "X_test = list(steam[\"reviews_processed\"][200001:])\n",
    "Y_train = list(steam[\"recommendation\"][200001:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method KNN_NLC_Classifer.similarity_score of <__main__.KNN_NLC_Classifer object at 0x1d7830790>>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_corpus \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m Y_pred \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[58], line 21\u001b[0m, in \u001b[0;36mKNN_NLC_Classifer.predict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m     19\u001b[0m max_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_train)):\n\u001b[0;32m---> 21\u001b[0m     temp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_similarity(x_test[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_train[j])\n\u001b[1;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m temp \u001b[39m>\u001b[39m max_sim:\n\u001b[1;32m     23\u001b[0m         max_sim \u001b[39m=\u001b[39m temp\n",
      "Cell \u001b[0;32mIn[58], line 97\u001b[0m, in \u001b[0;36mKNN_NLC_Classifer.document_similarity\u001b[0;34m(self, doc1, doc2)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdocument_similarity\u001b[39m(\u001b[39mself\u001b[39m,doc1, doc2):\n\u001b[1;32m     95\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     synsets1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc_to_synsets(doc1)\n\u001b[1;32m     98\u001b[0m     synsets2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_to_synsets(doc2)\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilarity_score(synsets1, synsets2) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilarity_score(synsets2, synsets1)) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[58], line 49\u001b[0m, in \u001b[0;36mKNN_NLC_Classifer.doc_to_synsets\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdoc_to_synsets\u001b[39m(\u001b[39mself\u001b[39m, doc):\n\u001b[1;32m     37\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m        Returns a list of synsets in document.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m        Tokenizes and tags the words in the document doc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m        list of synsets\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     tokens \u001b[39m=\u001b[39m word_tokenize(doc\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     51\u001b[0m     l \u001b[39m=\u001b[39m []\n\u001b[1;32m     52\u001b[0m     tags \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag([tokens[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tokens) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m nltk\u001b[39m.\u001b[39mpos_tag(tokens)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "classifier = KNN_Classifer(k=1, distance_type='path')\n",
    "classifier.fit(X_train, Y_train)\n",
    "print(classifier.similarity_score)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
